{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (42000, 785)\nTest set shape: (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "#X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "train_orig = pd.read_csv('train.csv')\n",
    "test_orig = pd.read_csv('test.csv')\n",
    "print('Train set shape:',  train_orig.shape)\n",
    "print('Test set shape:',  test_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 784), (42000,), (28000, 784))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_orig = train_orig.iloc[:,1:]\n",
    "Y_train_orig = train_orig.iloc[:,0]\n",
    "X_test_orig = test_orig\n",
    "\n",
    "X_train_orig.shape, Y_train_orig.shape, X_test_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33600, 784), (8400, 784), (33600,), (8400,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_orig, Y_train_orig, test_size=0.20, random_state=42)\n",
    "\n",
    "X_train.shape, X_val.shape, Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "X_train = X_train.T/255.\n",
    "X_val = X_val.T/255.\n",
    "X_test = X_test_orig.T/255.\n",
    "# One-hot encode labels\n",
    "Y_train = pd.get_dummies(Y_train).T\n",
    "Y_val = pd.get_dummies(Y_val).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 33600\nnumber of validation examples = 8400\nnumber of test examples = 28000\nX_train shape: (784, 33600)\nX_val shape: (784, 8400)\nY_train shape: (10, 33600)\nY_val shape: (10, 8400)\nX_test shape: (784, 28000)\n"
     ]
    }
   ],
   "source": [
    "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print (\"number of validation examples = \" + str(X_val.shape[1]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"X_val shape: \" + str(X_val.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"Y_val shape: \" + str(Y_val.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(n_x, None), name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(n_y, None), name='Y')\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "        \n",
    "    W1 = tf.get_variable('W1', [100, 784], initializer= tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b1 = tf.get_variable('b1', [100,1], initializer= tf.zeros_initializer())\n",
    "    W2 = tf.get_variable('W2', [50,100], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b2 = tf.get_variable('b2', [50,1], initializer= tf.zeros_initializer())\n",
    "    W3 = tf.get_variable('W3', [10,50], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b3 = tf.get_variable('b3', [10,1], initializer= tf.zeros_initializer())\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, keep_prob):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.matmul(W1, X) + b1                    # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                           # A1 = relu(Z1)\n",
    "    A1 = tf.nn.dropout(A1, keep_prob)\n",
    "    Z2 = tf.matmul(W2, A1) + b2                   # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                           # A2 = relu(Z2)\n",
    "    A2 = tf.nn.dropout(A2, keep_prob)\n",
    "    Z3 = tf.matmul(W3, A2) + b3                   # Z3 = np.dot(W3,Z2) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [784, None])\n",
    "    \n",
    "    z3 = forward_propagation_for_predict(x, params)\n",
    "    p = tf.argmax(z3)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_for_predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3'] \n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 700, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X, parameters, keep_prob)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y:minibatch_Y, \n",
    "                                                                            keep_prob:0.8})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print(\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print(\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train, keep_prob:1.0}))\n",
    "        print(\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test, keep_prob:1.0}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.115009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 100: 0.030470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 200: 0.013285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 300: 0.011700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 400: 0.008449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 500: 0.006801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 600: 0.005505\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xda65160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\nTest Accuracy: 0.972262\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train.values, Y_train.values, X_val.values, Y_val.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the neural network is clearly overfitting the training set, by adding dropout to the layers, I could increase the accuracy from 94% to 97%. Keeping in mind, that this is a \"normal\", densely connected network without convolution, it is a very good result. I expect that a convolutional NN would be able to reach a higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict(X_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, ..., 3, 9, 2], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns=['ImageId','Label'])\n",
    "submission['ImageId'] = list(range(1, X_test.shape[1]+1))\n",
    "submission['Label'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from Kaggle was: Your submission scored 0.97114\n"
     ]
    }
   ],
   "source": [
    "print('Result from Kaggle was: Your submission scored 0.97114')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}