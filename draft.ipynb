{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (42000, 785)\nTest set shape: (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "#X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "train_orig = pd.read_csv('train.csv')\n",
    "test_orig = pd.read_csv('test.csv')\n",
    "print('Train set shape:',  train_orig.shape)\n",
    "print('Test set shape:',  test_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig = train_orig.iloc[:,1:]\n",
    "Y_train_orig = train_orig.iloc[:,0]\n",
    "X_test_orig = test_orig\n",
    "\n",
    "X_train_orig.shape, Y_train_orig.shape, X_test_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33600, 784), (8400, 784), (33600,), (8400,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_orig, Y_train_orig, test_size=0.20, random_state=42)\n",
    "\n",
    "X_train.shape, X_val.shape, Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "X_train = X_train.T/255.\n",
    "X_val = X_val.T/255.\n",
    "X_test = X_test_orig.T/255.\n",
    "# One-hot encode labels\n",
    "Y_train = pd.get_dummies(Y_train).T\n",
    "Y_val = pd.get_dummies(Y_val).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 33600\nnumber of validation examples = 8400\nnumber of test examples = 28000\nX_train shape: (784, 33600)\nX_val shape: (784, 8400)\nY_train shape: (10, 33600)\nY_val shape: (10, 8400)\nX_test shape: (784, 28000)\n"
     ]
    }
   ],
   "source": [
    "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print (\"number of validation examples = \" + str(X_val.shape[1]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"X_val shape: \" + str(X_val.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"Y_val shape: \" + str(Y_val.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(n_x, None), name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(n_y, None), name='Y')\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "        \n",
    "    W1 = tf.get_variable('W1', [25, 784], initializer= tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b1 = tf.get_variable('b1', [25,1], initializer= tf.zeros_initializer())\n",
    "    W2 = tf.get_variable('W2', [12,25], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b2 = tf.get_variable('b2', [12,1], initializer= tf.zeros_initializer())\n",
    "    W3 = tf.get_variable('W3', [10,12], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b3 = tf.get_variable('b3', [10,1], initializer= tf.zeros_initializer())\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': <tf.Variable 'W1:0' shape=(25, 784) dtype=float32_ref>,\n 'W2': <tf.Variable 'W2:0' shape=(12, 25) dtype=float32_ref>,\n 'W3': <tf.Variable 'W3:0' shape=(10, 12) dtype=float32_ref>,\n 'b1': <tf.Variable 'b1:0' shape=(25, 1) dtype=float32_ref>,\n 'b2': <tf.Variable 'b2:0' shape=(12, 1) dtype=float32_ref>,\n 'b3': <tf.Variable 'b3:0' shape=(10, 1) dtype=float32_ref>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "initialize_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.matmul(W1, X) + b1                    # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                           # A1 = relu(Z1)\n",
    "    Z2 = tf.matmul(W2, A1) + b2                   # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                           # A2 = relu(Z2)\n",
    "    Z3 = tf.matmul(W3, A2) + b3                   # Z3 = np.dot(W3,Z2) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [784, None])\n",
    "    \n",
    "    z3 = forward_propagation_for_predict(x, params)\n",
    "    p = tf.argmax(z3)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "        \n",
    "    return prediction\n",
    "\n",
    "\n",
    "def forward_propagation_for_predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3'] \n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y:minibatch_Y})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print(\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print(\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print(\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.667696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 100: 0.064466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 200: 0.021057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 300: 0.004043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 400: 0.000352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 500: 0.000035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 600: 0.000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 700: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 800: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 900: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 1000: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 1100: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 1200: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 1300: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 1400: 0.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X2cXVV97/HPd2YyE5KZJCSZhJAHQAkCUkAdg1gUsJUG\nqo22qFDqszdipd5re9vS277Qtte+rNbbagFj1BixAtoKmtIIgi1gQTQDhkCAhBADSSRkEkIeIE+T\n/O4fe02yGeacsxOy55xJvu/Xa7/O2Ws/nN/KgfObtddeaysiMDMzq6Wp3gGYmdnQ4IRhZmaFOGGY\nmVkhThhmZlaIE4aZmRXihGFmZoU4YdhhTdIPJb2/3nGYHQ6cMKwUklZJ+s16xxERF0bEN+sdB4Ck\nOyV9ZBA+p03SPElbJK2T9Mc19v99SU9Kel7S9yWNLXouSWdKul/SC+n1zNy20yTdJmmDJA/4Ogw4\nYdiQJaml3jH0aaRYgE8D04HjgPOBP5M0c6AdJb0a+ArwXmAi8AJwbZFzSWoFfgD8C3A08E3gB6kc\nYDfwXeDDh65qVlcR4cXLIV+AVcBvVtj2NmAx8BxwL3B6btuVwBPAVuAR4J25bR8A7gH+EdgI/N9U\n9t/APwCbgF8CF+aOuRP4SO74avueANydPvsO4BrgXyrU4TxgDfDnwDrgW2Q/mrcAPen8twBT0v6f\nAfYAO4BtwNWp/GTgduBZYBnw7kPwb/8r4ILc+t8AN1bY9++A63PrrwR2AR21zgVcAKwFlNv+FDCz\n32ecmP3U1P+/Sy8vb3ELwwaVpNcA84CPAuPI/rpdIKkt7fIE8CZgNPDXwL9ImpQ7xVnASrK/hj+T\nK1sGjAc+B3xdkiqEUG3f64Gfp7g+TfZXdzXHAGPJ/vqeTdZi/0ZanwZsB64GiIi/BH4CXBER7RFx\nhaSRZMniemACcAlwraRTB/owSddKeq7CsiTtczQwCXgwd+iDwKsr1OHV+X0j4glgJ3BSgXO9GlgS\nKSsU+Cwb4pwwbLDNBr4SET+LiD2R9S/sBN4AEBH/GhG/ioi9EfEd4HFgRu74X0XEP0dEb0RsT2VP\nRsRXI2IP2WWRSWQJZSAD7itpGvB64KqI2BUR/w0sqFGXvcCnImJnRGyPiI0R8b2IeCEitpIltHOr\nHP82YFVEfCPV5xfA94B3DbRzRPxhRIypsJyedmtPr5tzh24BOirE0N5v3/z+tc5V7Vg7DDlh2GA7\nDviT/F/HwFTgWABJ75O0OLftNLLWQJ/VA5xzXd+biHghvW0fYL9q+x4LPJsrq/RZeT0RsaNvRdII\nSV9JHchbyC5vjZHUXOH444Cz+v1bXEbWcjlY29LrqFzZaLLLbJX2H9WvrG//WueqdqwdhpwwbLCt\nBj7T76/jERFxg6TjgK8CVwDjImIM8DCQv7xU1t02TwNjJY3IlU2tcUz/WP4EeBVwVkSMAt6cylVh\n/9XAXf3+Ldoj4mMDfZikOZK2VViWAkTEplSXM3KHngEsrVCHpfl9Jb0SaAWWFzjXUuD0fpf/Tq/y\nWTbEOWFYmYZJGp5bWsgSwuWSzlJmpKTfltQBjCT7Ue0BkPRBshZG6SLiSaAb+LSkVklnA28/wNN0\nkPVbPJduTf1Uv+3PAK/Ird9C1lfwXknD0vJ6SadUiPHylFAGWvL9BtcBfyXp6HSu/wHMrxDzt4G3\nS3pT6lP5W+CmdEmt1rnuJOvI/0S6/fYTZN/ffwKk73c4WQIi/TfQ11dlQ5AThpVpIdkPaN/y6Yjo\nJvvRuZrsTqIVZHcvERGPAF8Afkr24/prZHdFDZbLgLPZfwfWd8j6V4r6J+AoYANwH3Brv+1fBC6W\ntEnSl9KP8gVknd2/Irtc9vfAy/1R/RTZzQNPkv2ofy4i9sWSWiRvAoiIpcDlZIljPVnS/sMi54qI\nXcA7gPeR3fH2AeAdqRyyS27b2d/i2E52w4ENUXrxDQ5m1kfSd4DHIqJ/S8HsiOQWhlmSLge9UlJT\nGpw2C/h+veMyaxSljU6VNI/stsH1EfGS69CS/pTsEkBfHKcAnRHxrKRVZHda7AF6I6KrrDjNco4B\nbiIbh7EG+Fi61dXMKPGSlKQ3k912d91ACaPfvm8HPhkRb0nrq4CuiNhQSnBmZnbASrskFRF3k013\nUMSlwA1lxWJmZi9f3SdMS/e9zyS7975PAHdI2kM2KnhuleNnk40eZuTIka87+eSTywzXzOywcv/9\n92+IiM4i+9Y9YZDd635PRORbI+dExFpJE4DbJT2WWiwvkZLJXICurq7o7u4uP2Izs8OEpCeL7tsI\nd0ldQr/LURGxNr2uB27mxXMJmZlZHdQ1YUgaTTY52w9yZSPTqF/SyNMLyKaHMDOzOirzttobyJ4Z\nMF7SGrIRo8MAImJO2u2dwI8i4vncoROBm9P0NC1kc/X3HzFrZmaDrLSEERGXFthnPv3muImIlbx4\nsjMzM2sAjdCHYWZmQ4AThpmZFeKEYWZmhThhAP/848e5a3lPvcMwM2toThjAtXc+wT0rPG2VmVk1\nThhAk2DvXj8XxMysGicMQBLOF2Zm1TlhABIEzhhmZtU4YQBNEn5SrZlZdU4YZC2Mvc4YZmZVOWHg\nFoaZWRFOGKS7pJwxzMyqcsIAwHdJmZnV4oRB1sLAd0mZmVXlhEHWh7F3b72jMDNrbE4YuA/DzKwI\nJww80tvMrAgnDDzS28ysCCcMPA7DzKwIJww80tvMrAgnDNzCMDMrorSEIWmepPWSHq6w/TxJmyUt\nTstVuW0zJS2TtELSlWXFuP/z3MIwM6ulzBbGfGBmjX1+EhFnpuVvACQ1A9cAFwKnApdKOrXEOBG4\nhWFmVkNpCSMi7gaePYhDZwArImJlROwCbgRmHdLg+mmSfJeUmVkN9e7DeKOkJZJ+KOnVqWwysDq3\nz5pUNiBJsyV1S+ru6ek5qCA80tvMrLZ6JowHgGkRcTrwz8D3D+YkETE3Iroioquzs/OgAnEfhplZ\nbXVLGBGxJSK2pfcLgWGSxgNrgam5XaekstJI8gUpM7Ma6pYwJB0jSen9jBTLRmARMF3SCZJagUuA\nBWXG0iQItzDMzKpqKevEkm4AzgPGS1oDfAoYBhARc4CLgY9J6gW2A5dE9qvdK+kK4DagGZgXEUvL\nijOLFc8lZWZWQ2kJIyIurbH9auDqCtsWAgvLiGsg2cA9Zwwzs2rqfZdUQ/BstWZmtTlhkA3c811S\nZmbVOWHQ94hWMzOrxgmDNHDPLQwzs6qcMEh3SXmkt5lZVU4Y9A3ccwvDzKwaJwyyPgzfJWVmVp0T\nBh6HYWZWhBMGHultZlaEEwZuYZiZFeGEgUd6m5kV4YRB3yNanTHMzKpxwiBNb17vIMzMGpwTBh7p\nbWZWhBMGHultZlaEEwZ+RKuZWRFOGPgRrWZmRThhAMJ9GGZmtThhAE1N4HxhZladEwZ9A/ecMczM\nqnHCoG/gXr2jMDNrbKUlDEnzJK2X9HCF7ZdJWiLpIUn3Sjojt21VKl8sqbusGPs0+S4pM7Oaymxh\nzAdmVtn+S+DciPg14G+Buf22nx8RZ0ZEV0nx7ZM9D8Mpw8ysmpayThwRd0s6vsr2e3Or9wFTyoql\nFvdhmJnV1ih9GB8GfphbD+AOSfdLml3tQEmzJXVL6u7p6TmoD5fch2FmVktpLYyiJJ1PljDOyRWf\nExFrJU0Abpf0WETcPdDxETGXdDmrq6vroH72s+dhHMyRZmZHjrq2MCSdDnwNmBURG/vKI2Jtel0P\n3AzMKDUO3IdhZlZL3RKGpGnATcB7I2J5rnykpI6+98AFwIB3Wh0qnq3WzKy20i5JSboBOA8YL2kN\n8ClgGEBEzAGuAsYB10oC6E13RE0Ebk5lLcD1EXFrWXGCR3qbmRVR5l1Sl9bY/hHgIwOUrwTOeOkR\nZfIjWs3MammUu6TqyrPVmpnV5oSBR3qbmRXhhIFHepuZFeGEQRrp7U4MM7OqnDBII73rHYSZWYNz\nwsAjvc3MinDCwCO9zcyKcMIAmprcwjAzq8UJg6wPwy0MM7PqnDAA4RaGmVktThikkd6+T8rMrCon\nDPpmq613FGZmjc0JA/dhmJkV4YRBNtLb+cLMrDonDLI+DPCMtWZm1ThhkN0lBbgfw8ysCicM3MIw\nMyvCCYNspDe4hWFmVo0TRo7vlDIzq8wJg2wcBuA7pczMqigtYUiaJ2m9pIcrbJekL0laIWmJpNfm\nts2UtCxtu7KsGPvs68PwaG8zs4rKbGHMB2ZW2X4hMD0ts4EvA0hqBq5J208FLpV0aolxkhoY7sMw\nM6uitIQREXcDz1bZZRZwXWTuA8ZImgTMAFZExMqI2AXcmPYtTd8lKfdhmJlVVs8+jMnA6tz6mlRW\nqXxAkmZL6pbU3dPTc1CByH0YZmY1DflO74iYGxFdEdHV2dl5UOfwOAwzs9pa6vjZa4GpufUpqWxY\nhfLSpHzhPgwzsyrq2cJYALwv3S31BmBzRDwNLAKmSzpBUitwSdq3NH0D99zCMDOrrLQWhqQbgPOA\n8ZLWAJ8iaz0QEXOAhcBFwArgBeCDaVuvpCuA24BmYF5ELC0rzhQr4BaGmVk1pSWMiLi0xvYAPl5h\n20KyhDIo+i5JuYVhZlbZkO/0PhT2jfSucxxmZo3MCYP9d0l5HIaZWWVOGHikt5lZEU4Y5AfuOWOY\nmVXihIFnqzUzK8IJg/zAPWcMM7NKCiUMSe8qUjZUNaV/BecLM7PKirYw/qJg2ZDk2WrNzGqrOnBP\n0oVko7EnS/pSbtMooLfMwOrBd0mZmVVWa6T3r4Bu4HeA+3PlW4FPlhXUYOtrYXjonplZZVUTRkQ8\nCDwo6fqI2A0g6WhgakRsGowAB0OT55IyM6upaB/G7ZJGSRoLPAB8VdI/lhjXoJJHepuZ1VQ0YYyO\niC3A75I9VvUs4DfKC2tw7ZsaZG994zAza2RFE0ZLet72u4FbSoynLvaN9HYfhplZRUUTxt+QPZ/i\niYhYJOkVwOPlhTW49k9vXtcwzMwaWqHnYUTEvwL/mltfCfxeWUENNo/DMDOrrehI7ymSbpa0Pi3f\nkzSl7OAGi0d6m5nVVvSS1DfInqt9bFr+PZUdFuQWhplZTUUTRmdEfCMietMyH+gsMa5BtX/ywbqG\nYWbW0IomjI2S/kBSc1r+ANhYZmCDySO9zcxqK5owPkR2S+064GngYuADJcU06DzS28ystgO5rfb9\nEdEZERPIEshf1zpI0kxJyyStkHTlANv/VNLitDwsaU8aTY6kVZIeStu6D6RSB2rfSG9nDDOzigrd\nVgucnp87KiKelfSaagdIagauAd4KrAEWSVoQEY/kzvN54PNp/7cDn4yIZ3OnOT8iNhSM8aD1JQyn\nCzOzyoq2MJrSpIMApFZArWQzA1gRESsjYhdwIzCryv6XAjcUjOeQ8jgMM7PairYwvgD8VFLf4L13\nAZ+pccxkYHVufQ1w1kA7ShoBzASuyBUHcIekPcBXImJuhWNnA7MBpk2bViOkgXmkt5lZbUVHel+X\n+hHekop+N39p6RB4O3BPv8tR50TEWkkTyGbLfSwi7h4gtrnAXICurq6D+slvSrMPOmGYmVVWtIVB\nShAHkiTWAlNz61NS2UAuod/lqIhYm17XS7qZ7BLXSxLGodDk6c3NzGoq2odxMBYB0yWdIKmVLCks\n6L+TpNHAucAPcmUjJXX0vQcuAB4uL1T3YZiZ1VK4hXGgIqJX0hVks9w2A/MiYqmky9P2OWnXdwI/\niojnc4dPBG5OU3a0ANdHxK1lxdrku6TMzGoqLWEARMRCYGG/sjn91ucD8/uVrQTOKDO2vL67pMIt\nDDOzisq8JDVkyE/cMzOryQmDXAujznGYmTUyJwxyLQxfkjIzq8gJAxDuwzAzq8UJg/1P3PPcg2Zm\nlTlhkL9Lqs6BmJk1MCcM8k/cc8YwM6vECQM/09vMrAgnDPaP9DYzs8qcMHALw8ysCCcMcrPVeqS3\nmVlFThh4pLeZWRFOGHikt5lZEU4Y7O/D8EhvM7PKnDDIPQ/D+cLMrCInDPb3YXhqEDOzypww8Ehv\nM7MinDDI9WHUOQ4zs0bmhEG+D8Mpw8ysEicMciO93YlhZlZRqQlD0kxJyyStkHTlANvPk7RZ0uK0\nXFX02ENpXwujzA8xMxviWso6saRm4BrgrcAaYJGkBRHxSL9dfxIRbzvIYw9VrIDvkjIzq6bMFsYM\nYEVErIyIXcCNwKxBOPaAyX0YZmY1lZkwJgOrc+trUll/b5S0RNIPJb36AI89JPzEPTOz2kq7JFXQ\nA8C0iNgm6SLg+8D0AzmBpNnAbIBp06YdVBBNnkvKzKymMlsYa4GpufUpqWyfiNgSEdvS+4XAMEnj\nixybO8fciOiKiK7Ozs6DClS4D8PMrJYyE8YiYLqkEyS1ApcAC/I7SDpGqcdZ0owUz8Yixx5Knq3W\nzKy20i5JRUSvpCuA24BmYF5ELJV0edo+B7gY+JikXmA7cElkPc8DHltWrH19GGZmVlmpfRjpMtPC\nfmVzcu+vBq4uemxZ9rUwfE3KzKwij/TGs9WamRXhhEF+pLczhplZJU4YeKS3mVkRThiJ5JHeZmbV\nOGEkTZJHepuZVeGEkTTJ4zDMzKpxwkiE3IdhZlaFE0Yi+S4pM7NqnDAS92GYmVXnhJFIHultZlaN\nE0bSJPmClJlZFU4YybBmsbN3T73DMDNrWE4YydiRrTz7/K56h2Fm1rCcMJLx7W1s2OqEYWZWiRNG\nMr69jQ3P76x3GGZmDcsJIxnX3srGbW5hmJlV4oSRjG9vY/P23ezq3VvvUMzMGpITRjKuvRXAHd9m\nZhU4YSTjRrYBsGGb+zHMzAbihJF0dmQtDCcMM7OBOWEkfS0Md3ybmQ2s1IQhaaakZZJWSLpygO2X\nSVoi6SFJ90o6I7dtVSpfLKm7zDhhfx/GRt9aa2Y2oJayTiypGbgGeCuwBlgkaUFEPJLb7ZfAuRGx\nSdKFwFzgrNz28yNiQ1kx5rW3tdDW0sQGtzDMzAZUZgtjBrAiIlZGxC7gRmBWfoeIuDciNqXV+4Ap\nJcZTlSSmHH0UK3uer1cIZmYNrcyEMRlYnVtfk8oq+TDww9x6AHdIul/S7EoHSZotqVtSd09Pz8sK\n+NRjR/Po01te1jnMzA5XDdHpLel8soTx57nicyLiTOBC4OOS3jzQsRExNyK6IqKrs7PzZcVxyqQO\n1j63nc3bd7+s85iZHY7KTBhrgam59Smp7EUknQ58DZgVERv7yiNibXpdD9xMdomrVKdMGgXAY25l\nmJm9RJkJYxEwXdIJklqBS4AF+R0kTQNuAt4bEctz5SMldfS9By4AHi4xVgBOTQnjEScMM7OXKO0u\nqYjolXQFcBvQDMyLiKWSLk/b5wBXAeOAayUB9EZEFzARuDmVtQDXR8StZcXaZ0JHG+PbW1myZnPZ\nH2VmNuSUljAAImIhsLBf2Zzc+48AHxnguJXAGf3LyyaJc04cz93Le9i7N2hq0mCHYGbWsBqi07uR\nnH/yBDY+v4sH1zxX71DMzBqKE0Y/557USZPgPx9bX+9QzMwaihNGP2NGtPLrJ47n3+5fw+49fjaG\nmVkfJ4wBvP/s43l68w5+tPSZeodiZtYwnDAG8JaTJ3D8uBF86ceP0+tWhpkZ4IQxoKYm8WczT2bZ\nM1u5/udP1TscM7OG4IRRwYWnHcObpo/nM//xKA95XIaZmRNGJZL4p/ecyfj2Nj76rW4/ic/MjnhO\nGFWMa2/jK+99HRuf38WH5i9i0/N+VoaZHbmcMGo4bfJorr3stTy2bivvmftT1m/ZUe+QzMzqwgmj\ngN84ZSLzP/h61m7azjuuuYclHgVuZkcgJ4yC3vjK8Xzno2cjiXdeey+fu/UxduzeU++wzMwGjRPG\nATht8mj+4xPn8Luvmcy1dz7BRV/8CT99YmPtA83MDgNOGAdozIhWPv+uM7juQzPYtWcvl371Pt77\n9Z+xeLUvU5nZ4U0RUe8YDpmurq7o7u4etM/bvmsP37pvFV++8wk2vbCb817VyWVnHcf5r+qkpdm5\n2Mwan6T703OIau/rhPHybdvZyzf++5dcd9+T9GzdycRRbby7ayrv7prK1LEjBj0eM7OinDDqZPee\nvfznY+u58edPcefyHgDOOXE8v3PGsfzWaccwaviwusVmZjYQJ4wGsPa57Xx30Wpu+sUaVj+7nZYm\n0XX80Zx70gTOPamTUyZ1kB5Ba2ZWN04YDSQi+MXq5/jR0me4a3kPjz69BcieH/7mkzqZccJYXn/8\nWI4fN8IJxMwGnRNGA3tmyw7uWt7DXct7uGfFBp57YTcA49tb6TpuLK877mhOntTBq47poLO9zUnE\nzErlhDFE7N0brNywjUWrNrFo1bN0r9rEU8++sG/70SOGcdLELHmcNLGDk4/pYPrEDkYf5b4QMzs0\nDiRhtJQcyEzgi0Az8LWI+Gy/7UrbLwJeAD4QEQ8UOfZw0NQkTpzQwYkTOrh0xjQANmzbyfJ1W1n2\nzFaWP7OVZeu2ctMDa9m2s3ffcceMGs6xY4ZzzOjhTBw1nGNGZe8ndGSvx4wazlGtzfWqlpkdpkpL\nGJKagWuAtwJrgEWSFkTEI7ndLgSmp+Us4MvAWQWPPSyNb29j/IltvPHE8fvKIoJfbd6xL5E8/sw2\nnt68ncfWbeWuZT08v+ulU5SMGt6yL6GMGdFKx/AWOtpa6BjeQntbCx3Dh9E+PFs/algzrS1NtLU0\n0drczLAW0drcRGtLWpqbfGnMzEptYcwAVkTESgBJNwKzgPyP/izgusiui90naYykScDxBY49Ykhi\n8pijmDzmKM4/ecJLtm/b2cu6zTt4ZssO1m3ewbot+98/s3UnazZtZ+uO3WzZ0cuu3oN75GxrcxMt\nzaJJQuJFr03KYmwSiNx6U7aezzX5tJNPQi9JRwWOMbPM2BGtfPfys0v/nDITxmRgdW59DVkrotY+\nkwseC4Ck2cBsgGnTpr28iIeo9rYWTpzQzokT2mvuu7N3D9t29LJtZy9bd/SyZcdudu7ey87eveza\ns5fd6XVXb1r27H/t3bOXCNgbsDeCiNj3fm8ABHv37l/Ptu/vI8v3luW7zvr3okWFY16yo5kB0DG8\n1N6FfQbnU0oUEXOBuZB1etc5nIbX1tJMW3sz49rb6h2KmQ0xZSaMtcDU3PqUVFZkn2EFjjUzs0FU\n5gx5i4Dpkk6Q1ApcAizot88C4H3KvAHYHBFPFzzWzMwGUWktjIjolXQFcBvZrbHzImKppMvT9jnA\nQrJbaleQ3Vb7wWrHlhWrmZnV5oF7ZmZHsAMZuOeHNpiZWSFOGGZmVogThpmZFeKEYWZmhRxWnd6S\neoAnD/Lw8cCGQxhOPbkujedwqQe4Lo3qYOtyXER0FtnxsEoYL4ek7qJ3CjQ616XxHC71ANelUQ1G\nXXxJyszMCnHCMDOzQpww9ptb7wAOIdel8Rwu9QDXpVGVXhf3YZiZWSFuYZiZWSFOGGZmVsgRnzAk\nzZS0TNIKSVfWO54DJWmVpIckLZbUncrGSrpd0uPp9eh6xzkQSfMkrZf0cK6sYuyS/iJ9T8sk/VZ9\noh5Yhbp8WtLa9N0slnRRblsj12WqpP+S9IikpZL+ZyofUt9NlXoMue9F0nBJP5f0YKrLX6fywf1O\nIj1q80hcyKZOfwJ4BdAKPAicWu+4DrAOq4Dx/co+B1yZ3l8J/H2946wQ+5uB1wIP14odODV9P23A\nCel7a653HWrU5dPA/x5g30avyyTgtel9B7A8xTykvpsq9Rhy3wvZo+3b0/thwM+ANwz2d3KktzBm\nACsiYmVE7AJuBGbVOaZDYRbwzfT+m8A76hhLRRFxN/Bsv+JKsc8CboyInRHxS7JnqMwYlEALqFCX\nShq9Lk9HxAPp/VbgUWAyQ+y7qVKPShqyHgCR2ZZWh6UlGOTv5EhPGJOB1bn1NVT/D6oRBXCHpPsl\nzU5lEyN7ciHAOmBifUI7KJViH6rf1R9JWpIuWfVdLhgydZF0PPAasr9oh+x3068eMAS/F0nNkhYD\n64HbI2LQv5MjPWEcDs6JiDOBC4GPS3pzfmNk7dMhee/0UI49+TLZ5c4zgaeBL9Q3nAMjqR34HvC/\nImJLfttQ+m4GqMeQ/F4iYk/6f30KMEPSaf22l/6dHOkJYy0wNbc+JZUNGRGxNr2uB24ma3Y+I2kS\nQHpdX78ID1il2IfcdxURz6T/yfcCX2X/JYGGr4ukYWQ/st+OiJtS8ZD7bgaqx1D+XgAi4jngv4CZ\nDPJ3cqQnjEXAdEknSGoFLgEW1DmmwiSNlNTR9x64AHiYrA7vT7u9H/hBfSI8KJViXwBcIqlN0gnA\ndODndYivsL7/kZN3kn030OB1kSTg68CjEfH/cpuG1HdTqR5D8XuR1ClpTHp/FPBW4DEG+zupd+9/\nvRfgIrK7J54A/rLe8Rxg7K8guxPiQWBpX/zAOODHwOPAHcDYesdaIf4byC4J7Ca7xvrharEDf5m+\np2XAhfWOv0BdvgU8BCxJ/wNPGiJ1OYfs0sYSYHFaLhpq302Vegy57wU4HfhFivlh4KpUPqjfiacG\nMTOzQo70S1JmZlaQE4aZmRXihGFmZoU4YZiZWSFOGGZmVogThjU8Sfem1+Ml/f4hPvf/GeizyiLp\nHZKuKunc/6f2Xgd8zl+TNP9Qn9eGJt9Wa0OGpPPIZhl92wEc0xIRvVW2b4uI9kMRX8F47gV+JyI2\nvMzzvKReZdVF0h3AhyLiqUN9bhta3MKwhiepb5bOzwJvSs8w+GSajO3zkhalieQ+mvY/T9JPJC0A\nHkll308TNC7tm6RR0meBo9L5vp3/LGU+L+lhZc8beU/u3HdK+jdJj0n6dhpRjKTPKnv2whJJ/zBA\nPU4CdvYlC0nzJc2R1C1puaS3pfLC9cqde6C6/IGyZygslvQVSc19dZT0GWXPVrhP0sRU/q5U3wcl\n3Z07/b+TzYJgR7p6j2D04qXWAmxLr+cBt+TKZwN/ld63Ad1kc/+fBzwPnJDbd2x6PYpspOy4/LkH\n+KzfA24ne2bKROApsucrnAdsJpubpwn4KdmI4nFkI2r7Wu1jBqjHB4Ev5NbnA7em80wnGyE+/EDq\nNVDs6f0yqUgCAAACe0lEQVQpZD/0w9L6tcD70vsA3p7efy73WQ8Bk/vHD/w68O/1/u/AS/2XlqKJ\nxawBXQCcLunitD6a7Id3F/DzyJ4D0OcTkt6Z3k9N+22scu5zgBsiYg/ZBG93Aa8HtqRzrwFQNt30\n8cB9wA7g65JuAW4Z4JyTgJ5+Zd+NbBK8xyWtBE4+wHpV8hvA64BFqQF0FPsnptuVi+9+snmJAO4B\n5kv6LnDT/lOxHji2wGfaYc4Jw4YyAX8UEbe9qDDr63i+3/pvAmdHxAuS7iT7S/5g7cy93wO0RESv\npBlkP9QXA1cAb+l33HayH/+8/p2IQcF61SDgmxHxFwNs2x0RfZ+7h/Q7EBGXSzoL+G3gfkmvi4iN\nZP9W2wt+rh3G3IdhQ8lWskdt9rkN+JiyKayRdFKatbe/0cCmlCxOJnu0ZZ/dfcf38xPgPak/oZPs\nEawVZ/tU9syF0RGxEPgkcMYAuz0KnNiv7F2SmiS9kmwyyWUHUK/+8nX5MXCxpAnpHGMlHVftYEmv\njIifRcRVZC2hvumxT2L/jK52BHMLw4aSJcAeSQ+SXf//ItnloAdSx3MPAz+O9lbgckmPkv0g35fb\nNhdYIumBiLgsV34zcDbZTMAB/FlErEsJZyAdwA8kDSf76/6PB9jnbuALkpT7C/8pskQ0Crg8InZI\n+lrBevX3orpI+ivgR5KayGbR/TjwZJXjPy9peor/x6nuAOcD/1Hg8+0w59tqzQaRpC+SdSDfkcY3\n3BIR/1bnsCqS1AbcRfZkx4q3J9uRwZekzAbX3wEj6h3EAZgGXOlkYeAWhpmZFeQWhpmZFeKEYWZm\nhThhmJlZIU4YZmZWiBOGmZkV8v8BXPi1CAGIdo4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe3d15f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\nTest Accuracy: 0.945238\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train.values, Y_train.values, X_val.values, Y_val.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that I'm overfitting the training set. This means that additional tuning is needed, in particular regularization. I will try to do introduce drop-out to see if the results improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, ..., 3, 9, 2], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict(X_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, ..., 3, 9, 2], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns=['ImageId','Label'])\n",
    "submission['ImageId'] = list(range(1, X_test.shape[1]+1))\n",
    "submission['Label'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from Kaggle was: Your submission scored 0.94157, which is not an improvement of your best score. Keep trying!\n"
     ]
    }
   ],
   "source": [
    "print('Result from Kaggle was: Your submission scored 0.94157, which is not an improvement of your best score. Keep trying!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}